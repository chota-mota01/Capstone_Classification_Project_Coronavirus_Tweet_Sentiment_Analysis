{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chota-mota01/Capstone_Classification_Project_Coronavirus_Tweet_Sentiment_Analysis/blob/main/Coronavirus_Tweet_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Coronavirus Tweet Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project task involves constructing a classification model to determine the sentiment of corona virus tweets. The dataset comprises tweets gathered from Twitter, which have undergone manual tagging for sentiment analysis.\n",
        "\n",
        "The project was conducted individually, and after analysing the dataset, it was discovered that it contained 41157 rows and 6 columns. The 'location' feature has numerous null values. To avoid affecting the original data, a copy was made, and specified column was converted to appropriate data type. The analysis of the cleaned data provided valuable insights into sentiment analysis.\n",
        "\n",
        "The researcher utilized data visualization techniques employing libraries such as seaborn and matplotlib. Various types of graphs, including bar graphs, count plots, line charts, box plots, histogram plots, correlation heatmaps, and pair plots, were employed. These visualizations played a crucial role in simplifying complex data and enhancing its interpretability.\n",
        "\n",
        "In our analysis of COVID-19 tweets, we focused on the \"OriginalTweet\" and \"Sentiment\" columns, disregarding irrelevant columns like \"UserName\" and \"ScreenName.\" This streamlined our analysis pipeline, allowing us to extract relevant data efficiently. We used stemming and lemmatizing for text normalization.\n",
        "\n",
        "We explored five machine learning models, including Logistic Regression, Naive Bayes Classifier, Random Forest, KNN(K-Nearest Neighbors), and Decision Tree. 'Logistic Regression' is statistical method for binary classification tasks, 'Naive Bayes Classifier' is a probabilistic classifier based on Bayes' theorem, 'Random Forest' is an ensemble learning method using multiple decision trees, 'KNN (K-Nearest Neighbors)' is a non-parametric classification algorithm and 'Decision Tree' is a tree-like structure used for classification and regression tasks. Despite employing grid search cross-validation to optimize these models, we observed minimal improvements in test accuracy across all models.\n",
        "\n",
        "The accuracy score and classification report are crucial metrics influencing analysis. They offer insights into a model's performance and its ability to correctly classify instances, aiding in real-world applications. The Logistic Regression model with Grid Search CV and count vectorization emerged as the top performer, boasting an accuracy of 79.51% without signs of overfitting.\n",
        "\n",
        "Our analysis revealed that despite the COVID-19 pandemic's challenges, positive sentiments outweighed negative ones in the tweets. Nonetheless, a notable portion of negative sentiments persists, presenting opportunities for initiatives aimed at addressing public concerns and bolstering morale.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/chota-mota01/Capstone_Classification_Project_Coronavirus_Tweet_Sentiment_Analysis"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project entails the development of a classification model aimed at predicting the sentiment expressed in COVID-19-related tweets. The dataset comprises tweets sourced from Twitter, which have undergone manual sentiment tagging. To ensure privacy protection, both names and usernames have been anonymized through encoding.\n",
        "\n",
        "The primary objective is to leverage machine learning techniques to accurately classify the sentiment conveyed in COVID-19 tweets. By building and deploying a robust classification model, we aim to gain insights into public sentiment surrounding the pandemic, enabling organizations to better understand public perception, sentiment trends, and potential areas of concern or positivity. Through this analysis, we seek to contribute to a deeper understanding of the public discourse surrounding COVID-19 on social media platforms."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GroupKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pylab import rcParams\n",
        "from sklearn.metrics import f1_score\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Importing datetime modules\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Dataset\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Coronavirus Tweets.csv'\n",
        "covi_data = pd.read_csv(path,encoding='latin-1')"
      ],
      "metadata": {
        "id": "WZ9VpR7TRLQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# head() method returns first 5 rows of the dataset\n",
        "covi_data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If number is specified, head() returns specified number of first rows\n",
        "covi_data.head(7)"
      ],
      "metadata": {
        "id": "aT6-jUPLUQ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Last Look\n",
        "# tail() method returns last 5 rows of the dataset\n",
        "covi_data.tail()"
      ],
      "metadata": {
        "id": "NtERJzBsUV6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If number is specified, tail() returns specified number of last rows\n",
        "covi_data.tail(8)"
      ],
      "metadata": {
        "id": "X0HM5OMQUnp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "covi_data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "covi_data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns present in dataset\n",
        "list(covi_data.columns)"
      ],
      "metadata": {
        "id": "yvL7JdxJY-Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "covi_data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "covi_data.isna().sum().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Used isnull().sum method to view null value in each column\n",
        "covi_data.isnull().sum()"
      ],
      "metadata": {
        "id": "42K2JlHNak1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Check Null value by plotting Heatmap\n",
        "from pickle import FALSE\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(covi_data.isnull(),cbar=FALSE)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset given contains coronavirus tweet information. We need to analyze the important factors in the dataset for tweet sentiment analysis.\n",
        "\n",
        "The dataset has 41157 rows and 6 columns. The dataset contains 8590 missing/null values and 0 duplicate values. The null values are in column 'Location'.\n",
        "\n",
        "Using seaborn library, we have visualized the following missing/null values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "covi_data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "covi_data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Username** **-** Unique user-IDs\n",
        "* **ScreenName** **-**  Unique screen name of the user\n",
        "* **Location** **-** Location of the user\n",
        "* **TweetAt** **-** Date of the tweet\n",
        "* **OriginalTweet** **-** The real tweet\n",
        "* **Sentiment** **-** Sentiment of the tweet\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in covi_data:\n",
        "  print(covi_data[column].unique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of Unique Values for each variable.\n",
        "for col in covi_data:\n",
        "  print(\"Count of unique values in\",col,\"is\",covi_data[col].nunique(),\".\")"
      ],
      "metadata": {
        "id": "1bqb3ZQUfG2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of Location\n",
        "covi_data['Location'].value_counts()"
      ],
      "metadata": {
        "id": "wh-Yhxpn3LCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 15 Countries\n",
        "covi_data['Location'].value_counts().head(15)"
      ],
      "metadata": {
        "id": "cTH8ytbo-vHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of TweetAt\n",
        "covi_data['TweetAt'].value_counts()"
      ],
      "metadata": {
        "id": "3vYzSXcu5BPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of Sentiment\n",
        "covi_data['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "IKT0dUEN23Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create copy of dataset\n",
        "covi_df=covi_data.copy()\n",
        "covi_df.columns"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all null values in Location column by NA\n",
        "covi_df[\"Location\"].fillna(\"NA\",inplace=True)"
      ],
      "metadata": {
        "id": "uyk6k2BPk5zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting data type of date column\n",
        "covi_df['TweetAt'] = pd.to_datetime(covi_df['TweetAt'].apply(lambda x: datetime.strptime(x,'%d-%m-%Y')))"
      ],
      "metadata": {
        "id": "NTED6ljXKMp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df.info()"
      ],
      "metadata": {
        "id": "Onwqk6nGK-NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While analyzing dataset, we found many null values. Before manipulation of the data, we created a copy of the coronavirus tweet sentiment analysis dataset because of which the changes made in the duplicate dataset won't affect the original dataset.\n",
        "\n",
        "After creating duplicate dataset, we replaced the null values of location column with NA. We changed the datatype of TweetAt to datetime data type.\n",
        "\n",
        "The manipulations performed are for better visualization of the dataset."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Countplot of TweetAt"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Count of OriginalTweet with TweetAt\n",
        "plt.figure(figsize=(12,6))\n",
        "grp_tweetAt = covi_df.groupby('TweetAt').count()['OriginalTweet'].plot()\n",
        "plt.ylabel('Count')\n",
        "plt.title('Tweeting Date', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot represents the counts of the observation present in the categorical variable. It uses the concept of a bar chart for the visual depiction.\n",
        "\n",
        "I selected the specific chart to know the tweet date of the original tweet."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that maximum tweets are done between dates 16-23 with nearly 3500 words and minimum tweets are done between dates 28-30."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Histogram of Positive Sentiment"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Plot number of characters for Positive sentiment\n",
        "tweet_len=covi_df[covi_df['Sentiment']==\"Positive\"]['OriginalTweet'].str.len()\n",
        "plt.hist(tweet_len,color='green')\n",
        "plt.title('Positive Sentiments')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram consists of bars that show the frequency of data within certain intervals, known as bins. The height of each bar represents the frequency of data falling within that bin."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the number of characters for positive sentiment are between 10 to 350. 250-270 characters are used for positive sentiment for more than 2500 times."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Histplot of TweetAt with different Sentiments"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Plot Tweet date with different sentiments\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.histplot(x= \"TweetAt\",data=covi_df, hue=\"Sentiment\", multiple=\"stack\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Tweet Date of Sentiments\", fontweight='bold')\n",
        "plt.ylabel(\"TweetAt\",fontsize = 12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of visualization commonly used to display the distribution of a univariate dataset. It represents the frequency or count of observations falling within predefined intervals, called bins, by plotting bars whose heights correspond to these counts."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights found from the chart are that on 20-03-2020, when maximum tweet took place showing the maximum sentiment types. Among all the sentiments, positive sentiment dominates the most followed by the negative in second place. The least tweeting date is 28-03-2020 with less number of sentiments.\n",
        "The maximum extremely postive sentiments is observed on 25-03-2020, whereas extremely negative sentiment was mostly tweeted on 20-03-2020."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Histogram of Negative Sentiment"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Plot number of characters for Negative sentiment\n",
        "tweet_len=covi_df[covi_df['Sentiment']==\"Negative\"]['OriginalTweet'].str.len()\n",
        "plt.hist(tweet_len,color='brown')\n",
        "plt.title('Negative Sentiments')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram consists of bars that show the frequency of data within certain intervals, known as bins. The height of each bar represents the frequency of data falling within that bin."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the number of characters for negative sentiment are between 10 to 350. 250 characters are used for negative sentiment for more than 1750 times."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Countplot for top 15 Countries"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Top 15 Countries\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y=covi_df.Location, order = covi_df.Location.value_counts().iloc[1:16].index, palette ='rocket_r')\n",
        "plt.title('Top 15 locations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot represents the counts of the observation present in the categorical variable. It uses the concept of a bar chart for the visual depiction."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe the top 15 countries for tweet sentiment analysis. The maximum number of tweets are from London followed by United States and other countries."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Histogram of Neutral Sentiment"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Plot number of characters for Neutral sentiment\n",
        "tweet_len=covi_df[covi_df['Sentiment']==\"Neutral\"]['OriginalTweet'].str.len()\n",
        "plt.hist(tweet_len,color='yellow')\n",
        "plt.title('Neutral Sentiments')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram comprises bars representing the frequency of data within specific intervals, referred to as bins. Each bar's height corresponds to the frequency of data falling within that particular bin."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the number of characters for neutral sentiment are between 10 to 350. 110-140 characters are used for neutral sentiment for 1200 times."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Histogram of Extremely Positive & Extremely Negative Sentiments"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Plot number of characters for Extremely Positive & Extremely Negative Sentiments\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))\n",
        "tweet_len=covi_df[covi_df['Sentiment']==\"Extremely Positive\"]['OriginalTweet'].str.len()\n",
        "ax1.hist(tweet_len,color='darkmagenta')\n",
        "ax1.set_title('Extremely Positive Sentiments')\n",
        "\n",
        "tweet_len=covi_df[covi_df['Sentiment']==\"Extremely Negative\"]['OriginalTweet'].str.len()\n",
        "ax2.hist(tweet_len,color='grey')\n",
        "ax2.set_title('Extremely Negative Sentiments')\n",
        "\n",
        "\n",
        "fig.suptitle(\"Characters in Tweet Sentiment\", size=15,fontweight=\"bold\")\n",
        "# Showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram comprises bars representing the frequency of data within specific intervals, referred to as bins. Each bar's height corresponds to the frequency of data falling within that particular bin."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart is that the number of characters for extremely positive and extremely negative sentiments are between 10 to 350. 250 characters are used for extremely positive sentiment for nearly 1600 times and 250-300 characters are used for extremely positive sentiment for more than 1600 times."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Countplot of TweetAt"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Dates of Tweets\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='TweetAt', data=covi_df, palette ='icefire')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Date of Tweets\")\n",
        "plt.xlabel(\"TweetAt\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot represents the counts of the observation present in the categorical variable. It uses the concept of a bar chart for the visual depiction."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the chart are the maximum number of tweets are done on 20-03-2020 with nearly 3500 count and the minimum number of tweets are done on 28-03-2020."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Pie Chart on Sentiment"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Percentage of Sentiment\n",
        "covi_df.Sentiment.value_counts()\n",
        "covi_df['Sentiment'].value_counts().plot(kind='pie',\n",
        "                                         figsize=(15,6),\n",
        "                                         autopct=\"%.2f%%\",\n",
        "                                         startangle=90,\n",
        "                                         labels=['Positive','Negative','Neutral','Extremely Positive','Extremely Negative'],\n",
        "                                         colors=['pink','brown','r','g','y'],\n",
        "                                         explode=[0.04,0.04,0.04,0.04,0.04])\n",
        "plt.legend(title='Sentiment:')\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart compares the contribution of each part to the data. It is a circular statistical graphic which is divided into slices to illustrate numerical proportion."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following chart helps us understand that the positive and negative sentiments are fairly high with 27.75% and 24.10%."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Interactive pie plot in percentage for Top 15 locations"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "location_per = pd.DataFrame(covi_df['Location'].value_counts().sort_values(ascending=False))\n",
        "location_per = location_per.rename(columns={'Location':'count'})"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the interactive pie plot in percentage for Top 15 locations\n",
        "data = {\n",
        "   \"values\": location_per['count'][1:16],\n",
        "   \"labels\": location_per.index[1:16],\n",
        "   \"domain\": {\"column\": 0},\n",
        "   \"name\": \"Location Name\",\n",
        "   \"hoverinfo\":\"label+percent+name\",\n",
        "   \"hole\": .4,\n",
        "   \"type\": \"pie\"\n",
        "}\n",
        "layout = go.Layout(title=\"Percentage of Location\", legend=dict(x=0.1, y=1.0, orientation=\"v\"))\n",
        "data = [data]\n",
        "fig = go.Figure(data = data, layout = layout)\n",
        "fig.update_layout(title_x=0.5)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Xh8kXKaCSFIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart compares the contribution of each part to the data. It is a circular statistical graphic which is divided into slices to illustrate numerical proportion."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The interactive pie plot shows the percentage of Top 15 locations. London is the topmost location with 11.7%."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Barplot for Top 10 Reference Present in Tweets"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Find different Reference present in tweets with having Reference using @\n",
        "def Reference(text):\n",
        "    line=re.findall(r'(?<=@)\\w+',text)\n",
        "    return \" \".join(line)\n",
        "covi_df['Reference']=covi_df['OriginalTweet'].apply(lambda x:Reference(x))\n",
        "\n",
        "temp=covi_df['Reference'].value_counts()[:][1:11]\n",
        "temp =temp.to_frame().reset_index().rename(columns={'index':'Reference','Reference':'count'})\n",
        "\n",
        "# Ploting the bar plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=\"Reference\",y=\"count\", data = temp, palette=\"pastel\")\n",
        "plt.title(\"Top 10 Reference Present in Tweets\", fontweight='bold')"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar graphs are the pictorial representation of data in the form of vertical or horizontal rectangular bars, where the length of bars are proportional to the measure of data. It is fundamental visualization used for comparing different sets of data and shows the relationship between two axes."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following chart helps us with top 15 reference present in the tweets. 'realDonaldTrump' holds highest reference and 'narendramodi' holds lowest reference in the tweets."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Boxplot of Data"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Boxplot of numerical column of dataset\n",
        "col = list(covi_df.columns)\n",
        "\n",
        "covi_df[col].plot(kind='box', title='Boxplot of Data',color='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are a measure of how well the data is distributed in the dataset. These charts display ranges within variables measured. This includes the outliers, the median, the mode, and where the majority of the data points lie in the “box”."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset has 2 numerical columns. The insight found from the chart is that the user name and screen name has no outliers present."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(20,5))\n",
        "cor = sns.heatmap(covi_df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps are a type of plot that visualize the strength of relationships between numerical variables. Correlation plots are used to understand which variables are related to each other and the strength of this relationship."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numerical column is represented for visualization of correlation heatmap."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14- Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(covi_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical. Pairplot allows us to plot pairwise relationships between variables within a dataset.\n",
        "\n",
        "The specific chart consists of entire dataset with each variable plotted. The plots are in matrix in which column name represents y-axis and row name represents x-axis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot basically plots entire dataframe. As our data has 2 numerical columns, we can see the relation between them using pair plot."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "''' Replace all null values in Location column by NA '''"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "'''No outliers present in the data'''"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "'''Not required'''"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "bnPkSEiLA8nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "# import library\n",
        "import contractions\n",
        "\n",
        "def expand_contraction(text):\n",
        "  expand_contraction = contractions.fix(text)\n",
        "  return expand_contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df['RealTweet']=covi_df['OriginalTweet'].apply(expand_contraction)"
      ],
      "metadata": {
        "id": "8ED7qolXIULx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert OriginalTweet to Lowercase\n",
        "covi_df['RealTweet'] = covi_df['RealTweet'].str.lower()\n",
        "covi_df['RealTweet']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Define function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space,\n",
        "    # which in effect deletes the punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function to remove punctuation\n",
        "covi_df['RealTweet']=covi_df['RealTweet'].apply(remove_punctuation)\n",
        "covi_df.head(3)"
      ],
      "metadata": {
        "id": "WDHxm7owBrED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df['RealTweet'] = covi_df['RealTweet'].str.replace('http\\S+|www.\\S+', '', case=False)"
      ],
      "metadata": {
        "id": "g2zGAS8yOd1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write function for removing @user\n",
        "def remove_pattern(txt, pattern):\n",
        "    r = re.findall(pattern, txt)\n",
        "    for i in r:\n",
        "        txt = re.sub(i,'',txt)\n",
        "    return txt"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column with removed @user\n",
        "covi_df['RealTweet'] = np.vectorize(remove_pattern)(covi_df['RealTweet'], '@[\\w]*')\n",
        "covi_df.head(3)"
      ],
      "metadata": {
        "id": "oSr-6NVVJBHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters, numbers, punctuations\n",
        "covi_df['RealTweet'] = covi_df['RealTweet'].str.replace('[^a-zA-Z#]+',' ')\n",
        "covi_df.head(3)"
      ],
      "metadata": {
        "id": "7nRhfoqjNfUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# Extract stopwords\n",
        "nltk.download('stopwords')\n",
        "# Extract the stopwords from nltk library\n",
        "sw = stopwords.words('english')\n",
        "# Display the stopwords\n",
        "np.array(sw)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to remove stopwords\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "DGu0ExgvB6xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords using function\n",
        "covi_df['RealTweet']=covi_df['RealTweet'].apply(stopwords)\n",
        "covi_df.tail(4)"
      ],
      "metadata": {
        "id": "O0Jwsu1ECCOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "covi_df.head()"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "''' Tokenization is being take care of by Stemming'''\n",
        "#covi_df['RealTweet']=covi_df['RealTweet'].apply(lambda x:str(x).split())\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Stemming\n",
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in str(text).split( )]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df['Stem'] = covi_df['RealTweet'].apply(lambda x: stemming(x))\n",
        "covi_df.head(10)"
      ],
      "metadata": {
        "id": "71EDgmtVSUSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "covi_df['Lemm'] = covi_df['RealTweet'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x.split()])"
      ],
      "metadata": {
        "id": "U_m5Ge7QT2xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df.head(3)"
      ],
      "metadata": {
        "id": "rkbPJIEIXD_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used stemming and lemmatizing for text normalization. Stemming is a text processing technique used in natural language processing (NLP) to reduce words to their root or base form, known as the stem. It involves removing prefixes, suffixes, and other affixes from words to normalize them and group together words with the same root, even if they have different inflected forms. Lemmatizing is a text processing technique used in natural language processing (NLP) to reduce words to their base or dictionary form, known as the lemma. Unlike stemming, which simply removes prefixes and suffixes to obtain a word stem, lemmatizing uses language-specific dictionaries and morphological analysis to ensure that the resulting lemma is a valid word."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "covi_df['po_tag']=nltk.pos_tag((covi_df['RealTweet']))\n",
        "covi_df.head()"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df['Sentiment'] = covi_df['Sentiment'].replace(\"Extremely Positive\",\"Positive\")\n",
        "covi_df['Sentiment'] = covi_df['Sentiment'].replace(\"Extremely Negative\",\"Negative\")"
      ],
      "metadata": {
        "id": "Ntwsex1o0E4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covi_df['Sentiment'].value_counts().reset_index()\n",
        "covi_df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "yIRcXFU_1E7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Test Split\n",
        "#Assigning dependent and independent features\n",
        "x= covi_df['Lemm']\n",
        "y= covi_df['Sentiment']\n",
        "# Applying Train test split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,stratify=y,random_state=1)\n",
        "#Printing the result\n",
        "print(\" X Train : \", x_train.shape)\n",
        "print(\" X Test : \", x_test.shape)"
      ],
      "metadata": {
        "id": "sH5ViDOnoXqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words\n",
        "co_vect = CountVectorizer(binary=False,max_df=1.0,min_df=5,ngram_range=(1,2))\n",
        "co_x_train = co_vect.fit_transform(x_train.astype(str).str.strip())\n"
      ],
      "metadata": {
        "id": "xozgtbwz2cYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "tfidf_vect = TfidfVectorizer(use_idf=True,max_df=1.0,min_df=5,ngram_range=(1,2),sublinear_tf=True)\n",
        "tfidf_x_train = tfidf_vect.fit_transform(x_train.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "zJtVTTQ12n_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_x_train.shape"
      ],
      "metadata": {
        "id": "dtxcIIBx3hO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co_x_test = co_vect.transform(x_test.astype(str).str.strip())\n",
        "tfidf_x_test = tfidf_vect.transform(x_test.astype(str).str.strip())"
      ],
      "metadata": {
        "id": "9B82X64j31sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_x_test.shape"
      ],
      "metadata": {
        "id": "d3LN1p6m4Sue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words and TF-IDF are the techniques used for text vectorization. Bag of Words (BoW) represents text data as a matrix where rows correspond to documents and columns correspond to unique words in the corpus. TF-IDF (Term Frequency-Inverse Document Frequency) assigns weights to words based on their frequency in the document and inverse frequency in the corpus."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "''' As numerical features present in the data are not useful for Sentiment Analysis.\n",
        "So, data scaling is not performed on the dataset.'''"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "'''Splitting done before text vectorization'''\n",
        "'''Splitting the data before text vectorization is essential for preventing data\n",
        "leakage, ensuring realistic evaluation, simulating real-world deployment scenarios,\n",
        "and enabling techniques like cross-validation for model assessment and hyperparameter tuning.'''\n",
        "#checking splitted data\n",
        "print(x_train.head())\n",
        "y_train.head()"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used 80:20 splitting ratio for sufficient training data, reasonable testing data and balance between bias and variance."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Negative', 'Neutral', 'Positive']"
      ],
      "metadata": {
        "id": "koxib0oj9j-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - **Logistic Regression**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a statistical method used for binary classification tasks, where the outcome variable or target variable is categorical and has only two possible outcomes. It is a type of regression analysis that models the probability of a binary outcome by fitting the data to a logistic function."
      ],
      "metadata": {
        "id": "65NRgXKDR4o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Initializing model\n",
        "lor= LogisticRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "lor.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_lor_cv=lor.predict(co_x_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_lor_cv = accuracy_score(y_test,pred_lor_cv)\n",
        "print(\"Accuracy :\",(accuracy_lor_cv))"
      ],
      "metadata": {
        "id": "vmCtirqWBxbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_lor_cv))"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_lor_cv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with CV )', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "cCsfqPIADCoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Initializing model\n",
        "lr_cv= LogisticRegression()\n",
        "parameters = dict(penalty=['l1', 'l2'],C=[100, 10, 1.0, 0.1, 0.01])\n",
        "\n",
        "# Hyperparameter tuning by GridserchCV\n",
        "lr_gcv = GridSearchCV(lr_cv,parameters)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_gcv.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_lr_gcv=lr_gcv.predict(co_x_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_lr_gcv = accuracy_score(y_test,pred_lr_gcv)\n",
        "print(\"Accuracy :\",(accuracy_lr_gcv))"
      ],
      "metadata": {
        "id": "KGc-iMoiKlxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_lr_gcv))"
      ],
      "metadata": {
        "id": "NS49ST2kKpRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_lr_gcv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Logistic Regression with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "nJtk4gU2KtQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch cross-validation is a valuable technique for evaluating and optimizing linear regression models, ensuring that they generalize well to new data and provide reliable predictions. It helps improve the robustness and generalization ability of the model, leading to more accurate and reliable results in real-world applications."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score improved after applying GridSearchCV from 0.7831 to 0.7951."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2- **Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes Classifier is a probabilistic machine learning model based on Bayes' theorem with an assumption of independence among features. It is commonly used for classification tasks, particularly in text classification and spam filtering. Naive Bayes Classifier remains a popular choice for classification tasks, especially in scenarios where the assumption holds reasonably well and computational efficiency is important."
      ],
      "metadata": {
        "id": "K-JA96JJTKMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "nb_clf = MultinomialNB()\n",
        "\n",
        "# Fit the Algorithm\n",
        "nb_clf.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_nb_clf = nb_clf.predict(co_x_test)"
      ],
      "metadata": {
        "id": "TV2lxnwyEdhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_nb_clf = accuracy_score(y_test,pred_nb_clf)\n",
        "print(\"Accuracy :\",(accuracy_nb_clf))"
      ],
      "metadata": {
        "id": "PrZ9cUjlEeb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label = ['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_nb_clf))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_nb_clf))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Naive Bayes Classifier with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "aWventfYElea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "nb_clf = MultinomialNB()\n",
        "parameters = dict(alpha=[100, 10, 1.0, 0.1, 0.01], fit_prior=[True,False])\n",
        "\n",
        "#Hyperparameter tuning by GridserchCV\n",
        "nb_gcv = GridSearchCV(nb_clf,parameters)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "nb_gcv.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_nb_gcv = nb_gcv.predict(co_x_test)"
      ],
      "metadata": {
        "id": "-t78EvLTOoOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_nb_gcv = accuracy_score(y_test,pred_nb_gcv)\n",
        "print(\"Accuracy :\",(accuracy_nb_gcv))"
      ],
      "metadata": {
        "id": "nhfDMY3xRKkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_nb_gcv))"
      ],
      "metadata": {
        "id": "Qn1yN8DkRiFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch cross-validation is a valuable technique for evaluating and optimizing linear regression models, ensuring that they generalize well to new data and provide reliable predictions. It helps improve the robustness and generalization ability of the model, leading to more accurate and reliable results in real-world applications."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying grid search cross-validation, there appears to be no significant improvement in the accuracy score."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - **Random Forest Classifier**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier is a popular ensemble learning algorithm that combines the power of multiple decision trees to make predictions. Random Forest Classifier is widely used in various machine learning applications, including classification, regression, and anomaly detection. It is known for its robustness, accuracy, and ease of use, making it a popular choice among practitioners and researchers. Proper tuning of hyperparameters such as the number of trees and maximum depth of trees is important for optimizing the performance of Random Forest models."
      ],
      "metadata": {
        "id": "ihlNB_Q1VEDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_clf.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_rf_clf = rf_clf.predict(co_x_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_rf_clf = accuracy_score(y_test,pred_rf_clf)\n",
        "print(\"Accuracy :\",(accuracy_rf_clf))"
      ],
      "metadata": {
        "id": "GQdpAfysL_8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_rf_clf))"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf2= (confusion_matrix(y_test,pred_rf_clf))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf2, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Random Forest with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "PgnjD3vrMUPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# creating param dict to check diffirent value of parameter\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid_rf = {'n_estimators': [50,80,100],\n",
        "                 'max_depth': [1,2,6],\n",
        "                 'min_samples_split':[10,20,30],\n",
        "                 'min_samples_leaf': [1,2,8]}\n",
        "\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "#fit the parameter with Cross Validation\n",
        "rf_rcv = RandomizedSearchCV(rf_clf, param_grid_rf,verbose= 3, scoring ='accuracy')\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_rcv.fit(co_x_train, y_train)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf_rcv.best_params_)\n",
        "print(rf_rcv.best_estimator_)"
      ],
      "metadata": {
        "id": "_eJwQ-57QAgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "pred_rf_rcv = rf_rcv.predict(co_x_test)"
      ],
      "metadata": {
        "id": "dREX1tAQQI38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_rf_rcv = accuracy_score(y_test,pred_rf_rcv)\n",
        "print(\"Accuracy :\",(accuracy_rf_rcv))"
      ],
      "metadata": {
        "id": "gJ5hIlPPQZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_rf_rcv))"
      ],
      "metadata": {
        "id": "o3Ahg5U0Qvdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf3= (confusion_matrix(y_test,pred_rf_rcv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf3, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Random Forest with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "2zAF3TALQ7qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedSearchCV is a technique used for hyperparameter optimization in machine learning, including for Random Forest models. It efficiently searches through a specified number of random combinations of hyperparameters and selects the combination that yields the best performance. This method saves time compared to exhaustive grid search by randomly sampling hyperparameters from predefined distributions. After fitting the RandomizedSearchCV object to the training data and selecting the best model based on cross-validated performance, the final model's performance is evaluated on the test set to obtain an unbiased estimate of its performance on unseen data."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score notably decreased after applying random search cross-validation on the random forest classifier."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - **K-Nearest Neighbors**"
      ],
      "metadata": {
        "id": "37maSfLVUcnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN is a versatile algorithm with various applications, including classification, regression, clustering, and outlier detection. It is particularly useful when the decision boundary is nonlinear or when the underlying data distribution is unknown. However, it may not perform well with high-dimensional data or imbalanced datasets. Proper preprocessing, feature scaling, and tuning of hyperparameters are essential for maximizing the performance of KNN."
      ],
      "metadata": {
        "id": "DhTEfYK7UvIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "# Initializing model\n",
        "knn_clf = KNeighborsClassifier()\n",
        "\n",
        "# Fit the Algorithm\n",
        "knn_clf.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_knn_clf=knn_clf.predict(co_x_test)"
      ],
      "metadata": {
        "id": "-5TkDD4KUcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_knn_clf = accuracy_score(y_test,pred_knn_clf)\n",
        "print(\"Accuracy :\",(accuracy_lor_cv))"
      ],
      "metadata": {
        "id": "oqBUna-EUcnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "q62zTYP3UcnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_knn_clf))"
      ],
      "metadata": {
        "id": "MM9pLUUWUcnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf4= (confusion_matrix(y_test,pred_knn_clf))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf4, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "y0E6rOrDUcnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "VSkIjN05UcnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Initializing model\n",
        "knn_clf= KNeighborsClassifier()\n",
        "\n",
        "param = {'n_neighbors': [1,2,3,4]}    # Different values for the number of neighbors\n",
        "knn_gcv = GridSearchCV(estimator=knn_clf,param_grid=param)\n"
      ],
      "metadata": {
        "id": "Q9T3zMUeUcnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Algorithm\n",
        "knn_gcv.fit(co_x_train,y_train)\n",
        "\n",
        "# Predict on the model\n",
        "pred_knn_gcv=knn_gcv.predict(co_x_test)"
      ],
      "metadata": {
        "id": "bqdMh0KKehrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_knn_gcv = accuracy_score(y_test,pred_knn_gcv)\n",
        "print(\"Accuracy :\",(accuracy_knn_gcv))"
      ],
      "metadata": {
        "id": "Kcxi64EbUcnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_knn_gcv))"
      ],
      "metadata": {
        "id": "b_02vlvyUcnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_knn_gcv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (KNN with CV)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "TwtTVLw_UcnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "drt5BH6rUcnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch cross-validation is a valuable technique for evaluating and optimizing linear regression models, ensuring that they generalize well to new data and provide reliable predictions. It helps improve the robustness and generalization ability of the model, leading to more accurate and reliable results in real-world applications."
      ],
      "metadata": {
        "id": "zvGMXaEkUcnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "iL9ZrW1RUcnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing grid search cross-validation, the accuracy score of the KNN model experienced a notable decrease. This suggests that the hyperparameters selected by the grid search may not be optimal for the KNN algorithm when applied to your dataset."
      ],
      "metadata": {
        "id": "HSs8BNM6UcnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - **Decision Tree**"
      ],
      "metadata": {
        "id": "GCDPXyAwgLx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Tree Classifier algorithm builds a predictive model in the form of a tree structure. Each internal node in the tree represents a decision based on a feature, and each leaf node represents a class label or a decision. The algorithm partitions the feature space into regions, with each region corresponding to a leaf node in the tree. Decision trees are trained using a recursive partitioning algorithm that selects the best feature and split point at each node based on criteria such as Gini impurity or entropy."
      ],
      "metadata": {
        "id": "BLW9PoZKXPHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "# Initializing model\n",
        "dt_ti_clf=DecisionTreeClassifier()\n",
        "\n",
        "# Fit the data to model\n",
        "dt_ti_clf.fit(tfidf_x_train,y_train)\n",
        "\n",
        "# Prediction\n",
        "pred_dt_ti_clf=dt_ti_clf.predict(tfidf_x_test)"
      ],
      "metadata": {
        "id": "H1cEeKSigLx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy_dt_ti_clf = accuracy_score(y_test,pred_dt_ti_clf)\n",
        "print(\"Accuracy :\",(accuracy_dt_ti_clf))"
      ],
      "metadata": {
        "id": "GjxYMDVAgLx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "rsKu9XNpgLx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_dt_ti_clf))"
      ],
      "metadata": {
        "id": "qOTJDl4RgLx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Confussion matrix\n",
        "cf5= (confusion_matrix(y_test,pred_dt_ti_clf))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf5, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision Tree with TF-IDF)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "H_4TNP6YgLx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "fL_VwaDqgLx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Initializing model\n",
        "dt_ti_clf=DecisionTreeClassifier()\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [3, 5],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "dt_ti_gcv = GridSearchCV(estimator=dt_ti_clf, param_grid=param_grid, scoring='accuracy', n_jobs=-1)\n",
        "dt_ti_gcv.fit(tfidf_x_train,y_train)\n",
        "\n",
        "#prediction\n",
        "pred_dt_ti_gcv=dt_ti_gcv.predict(tfidf_x_test)"
      ],
      "metadata": {
        "id": "iTPjAQ-KgLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_dt_ti_gcv = accuracy_score(y_test,pred_dt_ti_gcv)\n",
        "print(\"Accuracy :\",(accuracy_dt_ti_gcv))"
      ],
      "metadata": {
        "id": "XMEoEtHUgLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report of Performance metrics\n",
        "label=['neutral','positive','negative']\n",
        "print(classification_report(y_test,pred_dt_ti_gcv))"
      ],
      "metadata": {
        "id": "U-kl9qG2gLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Confussion matrix\n",
        "cf1= (confusion_matrix(y_test,pred_dt_ti_gcv))\n",
        "plt.figure(figsize=(8,5))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cf1, annot=True, fmt=\".0f\",ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels', fontsize=15)\n",
        "ax.set_ylabel('Actual labels', fontsize=15)\n",
        "ax.set_title('Confusion Matrix (Decision Tree with TF-IDF)', fontsize=20)\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "AzjLVg6VgLx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "IrM9vUOBgLx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearch cross-validation is a valuable technique for evaluating and optimizing linear regression models, ensuring that they generalize well to new data and provide reliable predictions. It helps improve the robustness and generalization ability of the model, leading to more accurate and reliable results in real-world applications."
      ],
      "metadata": {
        "id": "hiPw9TDhgLx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "SdoNdH8pgLx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying grid search cross-validation, there was a marginal reduction in the accuracy score of the Decision Tree model when utilizing TF-IDF vectorization. This indicates that the hyperparameters determined through grid search may not have perfectly aligned with the dataset's characteristics, resulting in a slight decline in performance."
      ],
      "metadata": {
        "id": "8dIJW0U1gLx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score and classification report are vital metrics that can greatly influence business decisions and outcomes. By accurately assessing the performance of a classification model, businesses can make informed decisions regarding various aspects of their operations. For instance, a high accuracy score and a detailed classification report provide valuable insights into the model's ability to correctly classify instances, helping businesses gauge the effectiveness of their predictive models in real-world scenarios."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression with GridSearchCV is the ML model choosed from the above created models with the accuracy of 79.51%. Logistic Regression with GridSearchCV is utilizing grid search cross-validation to optimize hyperparameters for logistic regression."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective is to leverage machine learning techniques to accurately classify the sentiment conveyed in COVID-19 tweets. We focused our analysis solely on the \"OriginalTweet\" and \"Sentiment\" columns, as columns like \"UserName\" and \"ScreenName\" do not provide meaningful insights for our analysis. These two columns contain the primary data we need to analyze sentiments expressed in the tweets, thereby streamlining our data processing and analysis pipeline. We used stemming and lemmatizing for text normalization.\n",
        "In our analysis of COVID-19 tweets, we explored five machine learning models, including Logistic Regression, Naive Bayes Classifier, Random Forest, KNN, and Decision Tree. Despite implementing grid search cross-validation to optimize the models, we did not observe significant improvements in the test accuracy across all models.\n",
        "\n",
        "However, the Logistic Regression model with Grid Search CV and count vectorization stood out, achieving an accuracy of 79.51%. This model demonstrated robust performance without signs of overfitting, indicating its suitability for deployment.\n",
        "\n",
        "Our analysis revealed that despite the challenging circumstances of the COVID-19 pandemic, positive sentiments outweighed negative ones in the tweets. Nonetheless, a substantial portion of negative sentiments persists, presenting opportunities for government agencies, NGOs, and other entities to implement initiatives aimed at boosting public morale and addressing concerns.\n",
        "\n",
        "Looking ahead, repeating the analysis in the future and comparing it with the present sentiment analysis could provide valuable insights into the effectiveness of such initiatives over time. This iterative approach allows for ongoing assessment and adjustment of strategies to better address evolving sentiments and needs during unprecedented situations like the COVID-19 pandemic."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}